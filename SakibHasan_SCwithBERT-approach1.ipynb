{"cells":[{"metadata":{},"cell_type":"markdown","source":"Trying to undestand how BERT works, and how to implement a Sentiment CLassification fine tuning BERT.\n\nI have taken help from the following article:\n[Sentiment Classification](https://towardsml.com/2019/09/17/bert-explained-a-complete-guide-with-theory-and-tutorial/)\n\nand also this [Kaggle Notebook](https://www.kaggle.com/houssemayed/imdb-sentiment-classification-with-bert)"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"!pip install transformers","execution_count":1,"outputs":[{"output_type":"stream","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.7/site-packages (2.9.0)\nRequirement already satisfied: sacremoses in /opt/conda/lib/python3.7/site-packages (from transformers) (0.0.43)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers) (3.0.10)\nRequirement already satisfied: tokenizers==0.7.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.7.0)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (2020.4.4)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from transformers) (1.18.1)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers) (4.45.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers) (2.23.0)\nRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.7/site-packages (from transformers) (0.1.86)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (0.14.1)\nRequirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (7.1.1)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (1.14.0)\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (1.24.3)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2020.4.5.1)\nRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2.9)\nRequirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (3.0.4)\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# Setup and imports\n\nLet's start with installing the transformers library and some required imports for the rest of the notebook"},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nfrom transformers import *\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.model_selection import train_test_split","execution_count":2,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data exploration & visualization"},{"metadata":{},"cell_type":"markdown","source":"In this case, I am using the IMDB reviews dataset given to use in the CODING ASSIGNMENTs which were also used in the resource link, which is also available from kaggle datasets through this link:\nhttps://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"../input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv\")\ndf['sentiment'] = df['sentiment'].apply(lambda x: 1 if x=='positive' else 0)\ndf.head()","execution_count":3,"outputs":[{"output_type":"execute_result","execution_count":3,"data":{"text/plain":"                                              review  sentiment\n0  One of the other reviewers has mentioned that ...          1\n1  A wonderful little production. <br /><br />The...          1\n2  I thought this was a wonderful way to spend ti...          1\n3  Basically there's a family where a little boy ...          0\n4  Petter Mattei's \"Love in the Time of Money\" is...          1","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review</th>\n      <th>sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>One of the other reviewers has mentioned that ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>I thought this was a wonderful way to spend ti...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Basically there's a family where a little boy ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#dataset dimensions\ndf.shape","execution_count":4,"outputs":[{"output_type":"execute_result","execution_count":4,"data":{"text/plain":"(50000, 2)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Labeled reviews barplot\nsns.countplot(df.sentiment)\nplt.xlabel('sentiments')","execution_count":5,"outputs":[{"output_type":"execute_result","execution_count":5,"data":{"text/plain":"Text(0.5, 0, 'sentiments')"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"metadata":{},"cell_type":"markdown","source":"As it's shown by the figure above the dataset is perfectly balanced with equal number of reviews text for the two classes.\n0 means that the review is negative and 1 for positive"},{"metadata":{},"cell_type":"markdown","source":"# BERT (Bidirectional Encoder Representations from Transformers)"},{"metadata":{},"cell_type":"markdown","source":"The Transformers library provides a wide variety of Transformer models (including BERT). It works with TensorFlow and PyTorch! It also includes prebuild tokenizers to do the heavy work required for bert model input. As far as I have understood from the resource [Link](https://towardsml.com/2019/09/17/bert-explained-a-complete-guide-with-theory-and-tutorial/), I am trying to implement SENTIMENT CLASSIFICATION with pre-trained model BERT as the understanding of this will be a huge help for our research project as it is based on the basics of BERT."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Selecting the bert-base-cased\nPRE_TRAINED_MODEL_NAME = '../input/bert-base-cased/'","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load pre-trained model tokenizer (vocabulary)\ntokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#I am checking to understand how Bertencoder is encoding a sentence with a sample string\nsample_txt = 'When was I last outside? I am stuck at home for 2 weeks.'\ntokens = tokenizer.tokenize(sample_txt)\ntoken_ids = tokenizer.convert_tokens_to_ids(tokens)\n\nprint(f' Sentence: {sample_txt}')\nprint(f'   Tokens: {tokens}')\nprint(f'Token IDs: {token_ids}')","execution_count":8,"outputs":[{"output_type":"stream","text":" Sentence: When was I last outside? I am stuck at home for 2 weeks.\n   Tokens: ['when', 'was', 'i', 'last', 'outside', '?', 'i', 'am', 'stuck', 'at', 'home', 'for', '2', 'weeks', '.']\nToken IDs: [1165, 1108, 178, 1314, 1796, 136, 178, 1821, 5342, 1120, 1313, 1111, 123, 2277, 119]\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"Some special tokens id: The following tokens are necessary to convert an input data or training/testing data suitable to work with the BERT based models."},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer.sep_token, tokenizer.sep_token_id","execution_count":9,"outputs":[{"output_type":"execute_result","execution_count":9,"data":{"text/plain":"('[SEP]', 102)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer.cls_token, tokenizer.cls_token_id","execution_count":10,"outputs":[{"output_type":"execute_result","execution_count":10,"data":{"text/plain":"('[CLS]', 101)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer.pad_token, tokenizer.pad_token_id","execution_count":11,"outputs":[{"output_type":"execute_result","execution_count":11,"data":{"text/plain":"('[PAD]', 0)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer.unk_token, tokenizer.unk_token_id","execution_count":12,"outputs":[{"output_type":"execute_result","execution_count":12,"data":{"text/plain":"('[UNK]', 100)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#To do all preprocessing I am specifying some parameters in the encod_plus() method of the tokenizer\nencoding = tokenizer.encode_plus(\n  sample_txt, #we are using our sample text to encode as necessary for the input to BERT\n  max_length=32,\n  add_special_tokens=True, # Add '[CLS]' and '[SEP]'\n  return_token_type_ids=False,\n  pad_to_max_length=True,\n  return_attention_mask=True,\n  return_tensors='pt',  # Return PyTorch tensors\n)\n\nencoding.keys()","execution_count":13,"outputs":[{"output_type":"execute_result","execution_count":13,"data":{"text/plain":"dict_keys(['input_ids', 'attention_mask'])"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#The tokens ids list\nprint(len(encoding['input_ids'][0]))\nencoding['input_ids'][0]","execution_count":14,"outputs":[{"output_type":"stream","text":"32\n","name":"stdout"},{"output_type":"execute_result","execution_count":14,"data":{"text/plain":"tensor([ 101, 1165, 1108,  178, 1314, 1796,  136,  178, 1821, 5342, 1120, 1313,\n        1111,  123, 2277,  119,  102,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0])"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#The attentions masked tokens\nprint(len(encoding['attention_mask'][0]))\nencoding['attention_mask']","execution_count":15,"outputs":[{"output_type":"stream","text":"32\n","name":"stdout"},{"output_type":"execute_result","execution_count":15,"data":{"text/plain":"tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0]])"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#let's see how the sentence is tokenized with bert tokenizer\ntokenizer.convert_ids_to_tokens(encoding['input_ids'][0])","execution_count":16,"outputs":[{"output_type":"execute_result","execution_count":16,"data":{"text/plain":"['[CLS]',\n 'when',\n 'was',\n 'i',\n 'last',\n 'outside',\n '?',\n 'i',\n 'am',\n 'stuck',\n 'at',\n 'home',\n 'for',\n '2',\n 'weeks',\n '.',\n '[SEP]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]',\n '[PAD]']"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"In order to get the suitable sequence length we need to take a look at all reviews length and then select the most appropriate one"},{"metadata":{"trusted":true},"cell_type":"code","source":"token_lens = []\n\n#looping through the dataset and making a list of length of tokens found in each of the reviews\nfor txt in df.review:\n  tokens = tokenizer.encode(txt, max_length=512)\n  token_lens.append(len(tokens))\nsns.distplot(token_lens)\nplt.xlim([0, 500]);\nplt.xlabel('Token count')","execution_count":17,"outputs":[{"output_type":"execute_result","execution_count":17,"data":{"text/plain":"Text(0.5, 0, 'Token count')"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"MAX_LEN = 200      #for not consuming much resources\nRANDOM_SEED = 42\ndevice = torch.device( 'cuda' if torch.cuda.is_available() else 'cpu' )","execution_count":18,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#doing the split of the dataset into training, validation and testing sets\ndf_train, df_test = train_test_split(df, test_size=0.1, random_state=RANDOM_SEED)\ndf_val, df_test = train_test_split(df_test, test_size=0.5, random_state=RANDOM_SEED)","execution_count":19,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.shape, df_val.shape, df_test.shape","execution_count":20,"outputs":[{"output_type":"execute_result","execution_count":20,"data":{"text/plain":"((45000, 2), (2500, 2), (2500, 2))"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"# Creating Dataset and Dataloader"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Dataset Creation class for creating the specified format dataset for input to BERT\nclass IMDBDataset(Dataset):\n\n  def __init__(self, reviews, sentiments, tokenizer, max_len):\n    self.reviews = reviews\n    self.sentiments = sentiments\n    self.tokenizer = tokenizer\n    self.max_len = max_len\n  \n  def __len__(self):\n    return len(self.reviews)\n  \n  def __getitem__(self, item):\n    review = str(self.reviews[item])\n    sentiment = self.sentiments[item]\n\n    encoding = self.tokenizer.encode_plus(\n      review,\n      add_special_tokens=True,\n      max_length=self.max_len,\n      return_token_type_ids=False,\n      pad_to_max_length=True,\n      return_attention_mask=True,\n      return_tensors='pt',\n    )\n\n    return {\n      'review': review,\n      'input_ids': encoding['input_ids'].flatten(),\n      'attention_mask': encoding['attention_mask'].flatten(),\n      'sentiments': torch.tensor(sentiment, dtype=torch.long)\n    }","execution_count":22,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#function to create the dataset calling the IMDBDataset class\ndef create_data_loader(df, tokenizer, max_len, batch_size):\n  ds = IMDBDataset(\n    reviews=df.review.to_numpy(), #separating the reviews\n    sentiments=df.sentiment.to_numpy(), #separating the sentiments\n    tokenizer=tokenizer, #BERT tokenizer\n    max_len=max_len\n  )\n\n  return DataLoader(\n    ds,\n    batch_size=batch_size,\n    num_workers=4\n  )","execution_count":23,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"BATCH_SIZE = 16\n\ntrain_data_loader = create_data_loader(df_train, tokenizer, MAX_LEN, BATCH_SIZE)\nval_data_loader = create_data_loader(df_val, tokenizer, MAX_LEN, BATCH_SIZE)\ntest_data_loader = create_data_loader(df_test, tokenizer, MAX_LEN, BATCH_SIZE)","execution_count":24,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = next(iter(train_data_loader))\ndata.keys()","execution_count":25,"outputs":[{"output_type":"execute_result","execution_count":25,"data":{"text/plain":"dict_keys(['review', 'input_ids', 'attention_mask', 'sentiments'])"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(data['input_ids'].shape)\nprint(data['attention_mask'].shape)\nprint(data['sentiments'].shape)","execution_count":26,"outputs":[{"output_type":"stream","text":"torch.Size([16, 200])\ntorch.Size([16, 200])\ntorch.Size([16])\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"As you see here we get a training data tensor of size (batch_size * max_length) "},{"metadata":{},"cell_type":"markdown","source":"# Reviews Classification with BERT and Hugging Face"},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np","execution_count":27,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Sentiment CLassifier model\nclass IMDBClassifier(nn.Module):\n\n  def __init__(self, n_classes):\n    super(IMDBClassifier, self).__init__()\n    self.bert = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\n    self.drop = nn.Dropout(p=0.3)\n    self.out = nn.Linear(self.bert.config.hidden_size, n_classes)\n  \n  def forward(self, input_ids, attention_mask):\n    _, pooled_output = self.bert(\n      input_ids=input_ids,\n      attention_mask=attention_mask\n    )\n    output = self.drop(pooled_output)\n    return self.out(output)","execution_count":28,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = IMDBClassifier(len(df['sentiment'].unique()))\nmodel = model.to(device)","execution_count":29,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"input_ids = data['input_ids'].to(device)\nattention_mask = data['attention_mask'].to(device)\n\nprint(input_ids.shape) # batch size x seq length\nprint(attention_mask.shape) # batch size x seq length","execution_count":30,"outputs":[{"output_type":"stream","text":"torch.Size([16, 200])\ntorch.Size([16, 200])\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"F.softmax(model(input_ids, attention_mask), dim=1)","execution_count":31,"outputs":[{"output_type":"execute_result","execution_count":31,"data":{"text/plain":"tensor([[0.2306, 0.7694],\n        [0.2527, 0.7473],\n        [0.4190, 0.5810],\n        [0.1832, 0.8168],\n        [0.3022, 0.6978],\n        [0.2773, 0.7227],\n        [0.3677, 0.6323],\n        [0.3240, 0.6760],\n        [0.3619, 0.6381],\n        [0.1985, 0.8015],\n        [0.3312, 0.6688],\n        [0.3567, 0.6433],\n        [0.2987, 0.7013],\n        [0.2207, 0.7793],\n        [0.4596, 0.5404],\n        [0.3260, 0.6740]], grad_fn=<SoftmaxBackward>)"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"# Training (fine-tuning of BERT for classification task)\n\nHow to come up with all hyperparameters? The BERT authors in this [paper](https://arxiv.org/pdf/1810.04805.pdf) have some recommendations for fine-tuning:\n\n* Batch size: 16, 32\n* Learning rate (Adam): 5e-5, 3e-5, 2e-5\n* Number of epochs: 2, 3, 4"},{"metadata":{"trusted":true},"cell_type":"code","source":"EPOCHS = 4\n\noptimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)\ntotal_steps = len(train_data_loader) * EPOCHS\n\nscheduler = get_linear_schedule_with_warmup(\n  optimizer,\n  num_warmup_steps=0,\n  num_training_steps=total_steps\n)\n\nloss_fn = nn.CrossEntropyLoss().to(device)","execution_count":32,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#defining the training function\ndef train_epoch(\n  model, \n  data_loader, \n  loss_fn, \n  optimizer, \n  device, \n  scheduler, \n  n_examples\n):\n  model = model.train()\n\n  losses = []\n  correct_predictions = 0\n  \n  for d in data_loader:\n    input_ids = d[\"input_ids\"].to(device)\n    attention_mask = d[\"attention_mask\"].to(device)\n    sentiments = d[\"sentiments\"].to(device)\n\n    outputs = model(\n      input_ids=input_ids,\n      attention_mask=attention_mask\n    )\n\n    _, preds = torch.max(outputs, dim=1)\n    loss = loss_fn(outputs, sentiments)\n\n    correct_predictions += torch.sum(preds == sentiments)\n    losses.append(loss.item())\n\n    loss.backward()\n    nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n    optimizer.step()\n    scheduler.step()\n    optimizer.zero_grad()\n\n  return correct_predictions.double() / n_examples, np.mean(losses)","execution_count":33,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#defining the evaluation model\ndef eval_model(model, data_loader, loss_fn, device, n_examples):\n  model = model.eval()\n\n  losses = []\n  correct_predictions = 0\n\n  with torch.no_grad():\n    for d in data_loader:\n      input_ids = d[\"input_ids\"].to(device)\n      attention_mask = d[\"attention_mask\"].to(device)\n      sentiments = d[\"sentiments\"].to(device)\n\n      outputs = model(\n        input_ids=input_ids,\n        attention_mask=attention_mask\n      )\n      _, preds = torch.max(outputs, dim=1)\n\n      loss = loss_fn(outputs, sentiments)\n\n      correct_predictions += torch.sum(preds == sentiments)\n      losses.append(loss.item())\n\n  return correct_predictions.double() / n_examples, np.mean(losses)","execution_count":34,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"%%time\n\ntrain_a = []\ntrain_l = []\nval_a = []\nval_l = []\nbest_accuracy = 0\n\nfor epoch in range(EPOCHS):\n\n  print(f'Epoch {epoch + 1}/{EPOCHS}')\n  print('-' * 10)\n\n  train_acc, train_loss = train_epoch(\n    model,\n    train_data_loader,    \n    loss_fn, \n    optimizer, \n    device, \n    scheduler, \n    len(df_train)\n  )\n\n  print(f'Train loss {train_loss} accuracy {train_acc}')\n\n  val_acc, val_loss = eval_model(\n    model,\n    val_data_loader,\n    loss_fn, \n    device, \n    len(df_val)\n  )\n\n  print(f'Val   loss {val_loss} accuracy {val_acc}')\n  print()\n\n  train_a.append(train_acc)\n  train_l.append(train_loss)\n  val_a.append(val_acc)\n  val_l.append(val_loss)\n\n  if val_acc > best_accuracy:\n    torch.save(model.state_dict(), 'best_model_state.bin')\n    best_accuracy = val_acc","execution_count":35,"outputs":[{"output_type":"stream","text":"Epoch 1/4\n----------\n","name":"stdout"},{"output_type":"stream","text":"/opt/conda/conda-bld/pytorch_1587428190859/work/torch/csrc/utils/python_arg_parser.cpp:756: UserWarning: This overload of add_ is deprecated:\n\tadd_(Number alpha, Tensor other)\nConsider using one of the following signatures instead:\n\tadd_(Tensor other, *, Number alpha)\nProcess Process-5:\nProcess Process-6:\nTraceback (most recent call last):\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.7/multiprocessing/process.py\", line 300, in _bootstrap\n    util._exit_function()\nKeyboardInterrupt\n  File \"/opt/conda/lib/python3.7/multiprocessing/process.py\", line 300, in _bootstrap\n    util._exit_function()\n  File \"/opt/conda/lib/python3.7/multiprocessing/util.py\", line 337, in _exit_function\n    _run_finalizers()\n  File \"/opt/conda/lib/python3.7/multiprocessing/util.py\", line 277, in _run_finalizers\n    finalizer()\n  File \"/opt/conda/lib/python3.7/multiprocessing/util.py\", line 337, in _exit_function\n    _run_finalizers()\n  File \"/opt/conda/lib/python3.7/multiprocessing/util.py\", line 201, in __call__\n    res = self._callback(*self._args, **self._kwargs)\n  File \"/opt/conda/lib/python3.7/multiprocessing/util.py\", line 277, in _run_finalizers\n    finalizer()\n  File \"/opt/conda/lib/python3.7/multiprocessing/util.py\", line 201, in __call__\n    res = self._callback(*self._args, **self._kwargs)\n  File \"/opt/conda/lib/python3.7/multiprocessing/queues.py\", line 192, in _finalize_join\n    thread.join()\n  File \"/opt/conda/lib/python3.7/threading.py\", line 1044, in join\n    self._wait_for_tstate_lock()\n  File \"/opt/conda/lib/python3.7/threading.py\", line 1060, in _wait_for_tstate_lock\n    elif lock.acquire(block, timeout):\nKeyboardInterrupt\n  File \"/opt/conda/lib/python3.7/multiprocessing/queues.py\", line 192, in _finalize_join\n    thread.join()\n  File \"/opt/conda/lib/python3.7/threading.py\", line 1044, in join\n    self._wait_for_tstate_lock()\n  File \"/opt/conda/lib/python3.7/threading.py\", line 1060, in _wait_for_tstate_lock\n    elif lock.acquire(block, timeout):\nException ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f70446f6dd0>\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 962, in __del__\n    self._shutdown_workers()\n  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 942, in _shutdown_workers\n    w.join()\n  File \"/opt/conda/lib/python3.7/multiprocessing/process.py\", line 140, in join\n    res = self._popen.wait(timeout)\n  File \"/opt/conda/lib/python3.7/multiprocessing/popen_fork.py\", line 48, in wait\n    return self.poll(os.WNOHANG if timeout == 0.0 else 0)\n  File \"/opt/conda/lib/python3.7/multiprocessing/popen_fork.py\", line 28, in poll\n    pid, sts = os.waitpid(self.pid, flag)\nKeyboardInterrupt: \n","name":"stderr"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n","\u001b[0;32m<ipython-input-33-79b824d83237>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(model, data_loader, loss_fn, optimizer, device, scheduler, n_examples)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    196\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \"\"\"\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     99\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(train_a, label='train accuracy')\nplt.plot(val_a, label='validation accuracy')\n\nplt.title('Training history')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend()\nplt.ylim([0, 1]);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_acc, _ = eval_model(\n  model,\n  test_data_loader,\n  loss_fn,\n  device,\n  len(df_test)\n)\n\ntest_acc.item()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_predictions(model, data_loader):\n  model = model.eval()\n  \n  review = []\n  predictions = []\n  prediction_probs = []\n  real_values = []\n\n  with torch.no_grad():\n    for d in data_loader:\n\n      reviews = d[\"review\"]\n      input_ids = d[\"input_ids\"].to(device)\n      attention_mask = d[\"attention_mask\"].to(device)\n      sentiments = d[\"sentiments\"].to(device)\n\n      outputs = model(\n        input_ids=input_ids,\n        attention_mask=attention_mask\n      )\n      _, preds = torch.max(outputs, dim=1)\n\n      probs = F.softmax(outputs, dim=1)\n\n      review.extend(reviews)\n      predictions.extend(preds)\n      prediction_probs.extend(probs)\n      real_values.extend(sentiments)\n\n  predictions = torch.stack(predictions).cpu()\n  prediction_probs = torch.stack(prediction_probs).cpu()\n  real_values = torch.stack(real_values).cpu()\n  return review, predictions, prediction_probs, real_values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_review_texts, y_pred, y_pred_probs, y_test = get_predictions(\n  model,\n  test_data_loader\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Performance report "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nclass_names = ['negative', 'positive']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test, y_pred, target_names=class_names))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def show_confusion_matrix(confusion_matrix):\n  hmap = sns.heatmap(confusion_matrix, annot=True, fmt=\"d\", cmap=\"Blues\")\n  hmap.yaxis.set_ticklabels(hmap.yaxis.get_ticklabels(), rotation=0, ha='right')\n  hmap.xaxis.set_ticklabels(hmap.xaxis.get_ticklabels(), rotation=30, ha='right')\n  plt.ylabel('True sentiment')\n  plt.xlabel('Predicted sentiment')\n\ncm = confusion_matrix(y_test, y_pred)\ndf_cm = pd.DataFrame(cm, index=class_names, columns=class_names)\nshow_confusion_matrix(df_cm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"idx = 2\n\nreview_text = y_review_texts[idx]\ntrue_sentiment = y_test[idx]\npred_df = pd.DataFrame({\n  'class_names': class_names,\n  'values': y_pred_probs[idx]\n})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(review_text)\nprint()\nprint(f'True sentiment: {class_names[true_sentiment]}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.barplot(x='values', y='class_names', data=pred_df, orient='h')\nplt.ylabel('sentiment')\nplt.xlabel('probability')\nplt.xlim([0, 1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Predict on raw text"},{"metadata":{"trusted":true},"cell_type":"code","source":"review_text = \"The film is too terrible I hate the characters actions and there's a lot of language mistakes\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"encoded_review = tokenizer.encode_plus(\n  review_text,\n  max_length=MAX_LEN,\n  add_special_tokens=True,\n  return_token_type_ids=False,\n  pad_to_max_length=True,\n  return_attention_mask=True,\n  return_tensors='pt',\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"input_ids = encoded_review['input_ids'].to(device)\nattention_mask = encoded_review['attention_mask'].to(device)\n\noutput = model(input_ids, attention_mask)\n_, prediction = torch.max(output, dim=1)\n\nprint(f'Review text: {review_text}')\nprint(f'Sentiment  : {class_names[prediction]}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"If you like it help with a simple up clic"}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}